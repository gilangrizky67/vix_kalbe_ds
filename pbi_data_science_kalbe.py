# -*- coding: utf-8 -*-
"""PBI_Data Science Kalbe

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bFUDlYVF7hhQXO13U9_1CKOoL5Y1PIw2
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error,mean_squared_error
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt
from statsmodels.tsa.arima.model import ARIMA
from pandas.plotting import autocorrelation_plot

# df_customer = pd.read_csv('/content/Case Study - Customer.csv',delimiter=';')
# df_product = pd.read_csv('/content/Case Study - Product.csv',delimiter=';')
# df_store = pd.read_csv('/content/Case Study - Store.csv',delimiter=';')
# df_transaction = pd.read_csv('/content/Case Study - Transaction.csv',delimiter=';')
df_merge =  pd.read_csv('/content/MergeAfterPrepo.csv')

"""# Data Preparation

## Data Customer
"""

df_customer.head(5)

df_customer.info()

df_customer['Income'] = df_customer['Income'].replace('[,]' , '.',regex=True).astype('float')

df_customer.info()

df_customer.head(5)

df_customer.isnull().sum()

df_customer.dropna(inplace=True)

df_customer.isnull().sum()

df_customer['Marital Status'].unique()

df_customer.head(1)

df_customer.duplicated().sum()

"""## Data Product"""

df_product.head(5)

df_product.info()

df_product.isnull().sum()

df_product.duplicated().sum()

"""## Data Store"""

df_store.head(1)

df_store.info()

df_store['Latitude'] = df_store['Latitude'].replace('[,]','.',regex=True).astype('float')
df_store['Longitude'] = df_store['Longitude'].replace('[,]','.',regex=True).astype('float')

df_store.head(3)

df_store.info()

df_store.isnull().sum()

df_store.duplicated().sum()

"""## Data Transaction"""

df_transaction.head(1)

df_transaction.info()

df_transaction.isnull().sum()

df_transaction.duplicated().sum()

df_transaction['Date'] = pd.to_datetime(df_transaction['Date'])

"""## Data Merge"""

# df_merge = pd.merge(df_transaction,df_customer,on=('CustomerID'))
# df_merge = pd.merge(df_merge,df_product.drop(columns=[('Price')]),on=('ProductID'))
# df_merge = pd.merge(df_merge, df_store, on=['StoreID'])

df_merge

# df_merge.to_csv('PBIDataMerge.csv', index=False)
df_merge.info()

"""# After Data Cleansing"""

df_merge['Date'] = pd.to_datetime(df_merge['Date'])

df_merge.head(5)

df_regresi = df_merge.groupby('Date').agg({'Qty':'sum'}).reset_index()

df_regresi.info()



# df_merge.to_csv('MergeAfterPrepo.csv', index=False)
# df_regresi.to_csv('regresiAfterPrepo.csv', index=False)

decomposed = seasonal_decompose(df_regresi.set_index('Date'))

plt.figure(figsize=(15,10))

plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title('Trend')
plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonal')
plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title('Residual')

plt.tight_layout()

from statsmodels.tsa.stattools import adfuller
result = adfuller(df_regresi['Qty'])
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
  print('\t%s: %.3f' % (key, value))

"""Data Stasioner, karena p-value < 0.05"""

cut_off = round(df_regresi.shape[0] * 0.8)
df_train = df_regresi[:cut_off]
df_test = df_regresi[cut_off:].reset_index(drop=True)
df_train.shape,df_test.shape

df_train

df_test

plt.figure(figsize=(20,5))
sns.lineplot(data=df_train,x=df_train['Date'],y=df_train['Qty'])
sns.lineplot(data=df_test,x=df_test['Date'],y=df_test['Qty'])

autocorrelation_plot(df_regresi['Qty'])

from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(df_regresi.Qty.diff().dropna())

from statsmodels.graphics.tsaplots import plot_acf
plot_acf(df_regresi.Qty.diff().dropna())

def eval(y_actual, y_pred):
    print(f'RMSE Value {mean_squared_error(y_actual,y_pred,squared=False)}')
    print(f'MAE Value {mean_absolute_error(y_actual,y_pred)}')

df_train = df_train.set_index('Date')
df_test = df_test.set_index('Date')

y = df_train['Qty']

ARIMAmodel = ARIMA(y,order = (70,0,1))
ARIMAmodel = ARIMAmodel.fit()

y_pred = ARIMAmodel.get_forecast(len(df_test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = ARIMAmodel.predict(start = y_pred_df.index[0],end = y_pred_df.index[-1])
y_pred_out = y_pred_df['predictions']
eval(df_test['Qty'],y_pred_out)

plt.figure(figsize=(20,5))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'],color='red')
plt.plot(y_pred_out,color='black',label='ARIMA Predictions')
plt.legend()

df_merge.head(5)

df_merge.corr()

df_cluster = df_merge.groupby(['CustomerID']).agg({
    'TransactionID': 'count',
    'Qty' : 'sum',
    # 'TotalAmount':'sum'
}).reset_index()

df_cluster.head()

data_cluster = df_cluster.drop(columns=['CustomerID'])
scaler = preprocessing.MinMaxScaler()


normalized_data = scaler.fit_transform(data_cluster)
# data_cluster_normalize = preprocessing.minmaxscaler(data_cluster)

data_cluster_normalize

normalized_data

K = range (2,8)
fits = []
score = []

for k in K:
    model = KMeans(n_clusters = k, random_state = 0, n_init='auto').fit(normalized_data)
    fits.append(model)
    score.append(silhouette_score(normalized_data,model.labels_, metric='euclidean'))

sns.lineplot(x=K,y=score)

fits[2]

df_cluster['cluster_label'] = fits[2].labels_

df_cluster.groupby(['cluster_label']).agg({
    'CustomerID' : 'count',
    'TransactionID': 'count',
    'Qty' : 'sum',
    # 'TotalAmount':'sum',
})

# Choose the number of clusters (4 in your case)
n_clusters = 4

# Fit the K-means model with the chosen number of clusters
chosen_model = fits[2]  # Since Python lists are 0-indexed

# Get cluster assignments for your data
cluster_assignments = chosen_model.labels_

# Calculate the centroid of each cluster
cluster_centers = chosen_model.cluster_centers_

# Create labels for clusters
cluster_labels = [f'Cluster {i + 1}' for i in range(n_clusters)]

# Plot the data points with cluster labels
for i in range(n_clusters):
    plt.scatter(
        normalized_data[cluster_assignments == i, 0],
        normalized_data[cluster_assignments == i, 1],
        label=cluster_labels[i],
        s=50
    )

# Plot cluster centers
for i, centroid in enumerate(cluster_centers):
    plt.scatter(centroid[0], centroid[1], c='black', marker='o', s=200)

# Add labels and legend

plt.title('Visualisasi')
plt.legend()
plt.show()